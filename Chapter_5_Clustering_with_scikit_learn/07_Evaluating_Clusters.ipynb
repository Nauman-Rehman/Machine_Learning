{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Evaluation metrics\n",
    "When we don't have access to any true cluster assignments (labels), the best we can do to evaluate clusters is to just take a look at them and see if they make sense with respect to the dataset and domain. However, if we do have access to the true cluster labels for the data observations, we can apply a number of metrics to evaluate our clustering algorithm.\n",
    "\n",
    "One popular evaluation metric is the adjusted Rand index. The regular Rand index gives a measurement of similarity between the true clustering assignments (true labels) and the predicted clustering assignments (predicted labels). The adjusted Rand index (ARI) is a corrected-for-chance version of the regular one, meaning that the score is adjusted so that random clustering assignments will not have a good score.\n",
    "\n",
    "The ARI value ranges from -1 to 1, inclusive. Negative scores represent bad labelings, random labelings will get a score near 0, and perfect labelings get a score of 1.\n",
    "\n",
    "In scikit-learn, ARI is implemented through the adjusted_rand_score function (part of the metrics module). It takes in two required arguments, the true cluster labels and the predicted cluster labels, and returns the ARI score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24242424242424243\n",
      "\n",
      "0.24242424242424243\n",
      "\n",
      "1.0\n",
      "\n",
      "1.0\n",
      "\n",
      "1.0\n",
      "\n",
      "-0.12903225806451613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "true_labels = np.array([0, 0, 0, 1, 1, 1])\n",
    "pred_labels = np.array([0, 0, 1, 1, 2, 2])\n",
    "\n",
    "ari = adjusted_rand_score(true_labels, pred_labels)\n",
    "print('{}\\n'.format(ari))\n",
    "\n",
    "# symmetric\n",
    "ari = adjusted_rand_score(pred_labels, true_labels)\n",
    "print('{}\\n'.format(ari))\n",
    "\n",
    "# Perfect labeling\n",
    "perf_labels = np.array([0, 0, 0, 1, 1, 1])\n",
    "ari = adjusted_rand_score(true_labels, perf_labels)\n",
    "print('{}\\n'.format(ari))\n",
    "\n",
    "# Perfect labeling, permuted\n",
    "permuted_labels = np.array([1, 1, 1, 0, 0, 0])\n",
    "ari = adjusted_rand_score(true_labels, permuted_labels)\n",
    "print('{}\\n'.format(ari))\n",
    "\n",
    "renamed_labels = np.array([1, 1, 1, 3, 3, 3])\n",
    "# Renamed labels to 1, 3\n",
    "ari = adjusted_rand_score(true_labels, renamed_labels)\n",
    "print('{}\\n'.format(ari))\n",
    "\n",
    "true_labels2 = np.array([0, 1, 2, 0, 3, 4, 5, 1])\n",
    "# Bad labeling\n",
    "pred_labels2 = np.array([1, 1, 0, 0, 2, 2, 2, 2])\n",
    "ari = adjusted_rand_score(true_labels2, pred_labels2)\n",
    "print('{}\\n'.format(ari))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the adjusted_rand_score function is symmetric. This means that changing the order of the arguments will not affect the score. Furthermore, permutations in the labeling or changing the label names (i.e. 0 and 1 vs. 1 and 3) does not affect the score.\n",
    "\n",
    "Another common clustering evaluation metric is adjusted mutual information (AMI). It is implemented in scikit-learn with the adjusted_mutual_info_score function (also part of the cluster module). Like adjusted_rand_score, the function is symmetric and oblivious to permutations and renamed labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2987924581708901\n",
      "\n",
      "0.2987924581708903\n",
      "\n",
      "1.0\n",
      "\n",
      "1.0\n",
      "\n",
      "1.0\n",
      "\n",
      "-0.16666666666666655\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "true_labels = np.array([0, 0, 0, 1, 1, 1])\n",
    "pred_labels = np.array([0, 0, 1, 1, 2, 2])\n",
    "\n",
    "ami = adjusted_mutual_info_score(true_labels, pred_labels)\n",
    "print('{}\\n'.format(ami))\n",
    "\n",
    "# symmetric\n",
    "ami = adjusted_mutual_info_score(pred_labels, true_labels)\n",
    "print('{}\\n'.format(ami))\n",
    "\n",
    "# Perfect labeling\n",
    "perf_labels = np.array([0, 0, 0, 1, 1, 1])\n",
    "ami = adjusted_mutual_info_score(true_labels, perf_labels)\n",
    "print('{}\\n'.format(ami))\n",
    "\n",
    "# Perfect labeling, permuted\n",
    "permuted_labels = np.array([1, 1, 1, 0, 0, 0])\n",
    "ami = adjusted_mutual_info_score(true_labels, permuted_labels)\n",
    "print('{}\\n'.format(ami))\n",
    "\n",
    "renamed_labels = np.array([1, 1, 1, 3, 3, 3])\n",
    "# Renamed labels to 1, 3\n",
    "ami = adjusted_mutual_info_score(true_labels, renamed_labels)\n",
    "print('{}\\n'.format(ami))\n",
    "\n",
    "true_labels2 = np.array([0, 1, 2, 0, 3, 4, 5, 1])\n",
    "# Bad labeling\n",
    "pred_labels2 = np.array([1, 1, 0, 0, 2, 2, 2, 2])\n",
    "ami = adjusted_mutual_info_score(true_labels2, pred_labels2)\n",
    "print('{}\\n'.format(ami))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ARI and AMI metrics are very similar. They both assign a score of 1.0 to perfect labelings, a score near 0.0 to random labelings, and negative scores to poor labelings.\n",
    "\n",
    "A general rule of thumb of when to use which: ARI is used when the true clusters are large and approximately equal sized, while AMI is used when the true clusters are unbalanced in size and there exist small clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
