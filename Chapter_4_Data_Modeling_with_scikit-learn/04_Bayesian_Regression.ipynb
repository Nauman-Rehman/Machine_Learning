{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (150, 4)\n",
      "\n",
      "Labels shape: (150,)\n",
      "\n",
      "Coefficients: array([ 0.        , -0.        ,  0.40811896,  0.        ])\n",
      "\n",
      "Intercept: -0.5337110569441172\n",
      "\n",
      "R2: 0.895821120274704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "# first --> pip install import_ipynb then write below lines to add files or modules made by us \n",
    "from IPython import get_ipython\n",
    "import import_ipynb\n",
    "get_ipython().run_line_magic('run','03_LASSO_Regression.ipynb')\n",
    "# but the above lines not only load or import the module but also run the code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Bayesian techniques\n",
    "So far, we've discussed hyperparameter optimization through cross-validation. Another way to optimize the hyperparameters of a regularized regression model is with Bayesian techniques.\n",
    "\n",
    "In Bayesian statistics, the main idea is to make certain assumptions about the probability distributions of a model's parameters before being fitted on data. These initial distribution assumptions are called priors for the model's parameters.\n",
    "\n",
    "In a Bayesian ridge regression model, there are two hyperparameters to optimize: α and λ. The α hyperparameter serves the same exact purpose as it does for regular ridge regression; namely, it acts as a scaling factor for the penalty term.\n",
    "\n",
    "The λ hyperparameter acts as the precision of the model's weights. Basically, the smaller the λ value, the greater the variance between the individual weight values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Hyperparameter priors\n",
    "Both the α and λ hyperparameters have gamma distribution priors, meaning we assume both values come from a gamma probability distribution.\n",
    "\n",
    "There's no need to know the specifics of a gamma distribution, other than the fact that it's a probability distribution defined by a shape parameter and scale parameter.\n",
    "\n",
    "Specifically, the α hyperparameter has prior:\n",
    "\n",
    "\n",
    "Γ(α \n",
    "1\n",
    "​\n",
    " ,α \n",
    "2\n",
    "​\n",
    " )\n",
    "\n",
    "and the λ hyperparameter has prior:\n",
    "\n",
    "Γ(λ \n",
    "1\n",
    "​\n",
    " ,λ \n",
    "2\n",
    "​\n",
    " )\n",
    "\n",
    "where Γ(k, θ) represents a gamma distribution with shape parameter k and scale parameter θ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Tuning the model\n",
    "When finding the optimal weight settings of a Bayesian ridge regression model for an input dataset, we also concurrently optimize the α and λ hyperparameters based on their prior distributions and the input data.\n",
    "\n",
    "This can all be done with the BayesianRidge object (part of the linear_model module). Like all the previous regression objects, this one can be initialized with no required arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (150, 4)\n",
      "\n",
      "Labels shape: (150,)\n",
      "\n",
      "Coefficients: array([-0.11362625, -0.03526763,  0.24468776,  0.57300547])\n",
      "\n",
      "Intercept: 0.16501980374055758\n",
      "\n",
      "R2: 0.9303174820768509\n",
      "\n",
      "Alpha: 20.975705701144673\n",
      "\n",
      "Lambda: 9.53356207176252\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predefined dataset from previous chapter\n",
    "print('Data shape: {}\\n'.format(data.shape))\n",
    "print('Labels shape: {}\\n'.format(labels.shape))\n",
    "\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.BayesianRidge()\n",
    "reg.fit(data, labels)\n",
    "print('Coefficients: {}\\n'.format(repr(reg.coef_)))\n",
    "print('Intercept: {}\\n'.format(reg.intercept_))\n",
    "print('R2: {}\\n'.format(reg.score(data, labels)))\n",
    "print('Alpha: {}\\n'.format(reg.alpha_))\n",
    "print('Lambda: {}\\n'.format(reg.lambda_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually specify the α1 and α2 gamma parameters for α with the alpha_1 and alpha_2 keyword arguments when initializing BayesianRidge. Similarly, we can manually set λ1 and λ2 with the lambda_1 and lambda_2 keyword arguments. The default value for each of the four gamma parameters is 10-6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
