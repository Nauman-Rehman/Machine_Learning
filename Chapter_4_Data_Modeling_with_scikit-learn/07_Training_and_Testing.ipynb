{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Training and testing sets \n",
    "We've discussed in depth how to fit a model on data and labels. However, once we fit the model, how do we evaluate it? It is a bad idea to evaluate a model solely on the same dataset it was fitted on, because the model's parameters are already tuned for that dataset. Instead, we need to split the original dataset into two datasets: one for training and one for testing.\n",
    "\n",
    "The training set is used for fitting the model on data (i.e. training the model), while the testing set is used for evaluating the model. Therefore, the training set is much larger than the testing set. Exactly how much larger depends on the application and requirements.\n",
    "\n",
    "Increasing the size of the training set will give more data for the model to be fitted on, which can increase the model's performance. However, because this decreases the size of the testing set, there's a higher chance that the testing set may not be representative of the original dataset (which can lead to inaccurate evaluation).\n",
    "\n",
    "In general, the testing set is around 10-30% of the original dataset, while the training set makes up the remaining 70-90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Splitting the dataset \n",
    "The scikit-learn library provides a nice utility function, called train_test_split (which is part of the model_selection module) that handles the dataset splitting for us.\n",
    "\n",
    "The code below demonstrates how to split a dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.5   0.77]\n",
      " [ 8.3   0.8 ]\n",
      " [10.1   0.4 ]\n",
      " [ 8.7   0.9 ]\n",
      " [ 9.3   0.8 ]\n",
      " [ 9.1   0.68]]\n",
      "\n",
      "[[ 7.7  0.9]\n",
      " [10.2  0.5]]\n",
      "\n",
      "[1.6 1.2 1.5 1.2 1.6 1.3]\n",
      "\n",
      "[1.1 1.4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = np.array([\n",
    "  [10.2 ,  0.5 ],\n",
    "  [ 8.7 ,  0.9 ],\n",
    "  [ 9.3 ,  0.8 ],\n",
    "  [10.1 ,  0.4 ],\n",
    "  [ 9.5 ,  0.77],\n",
    "  [ 9.1 ,  0.68],\n",
    "  [ 7.7 ,  0.9 ],\n",
    "  [ 8.3 ,  0.8 ]])\n",
    "labels = np.array(\n",
    "  [1.4, 1.2, 1.6, 1.5, 1.6, 1.3, 1.1, 1.2])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "split_dataset = train_test_split(data,labels)\n",
    "train_data = split_dataset[0]\n",
    "test_data = split_dataset[1]\n",
    "train_labels = split_dataset[2]\n",
    "test_labels = split_dataset[3]\n",
    "\n",
    "print('{}\\n'.format(train_data))\n",
    "print('{}\\n'.format(test_data))\n",
    "print('{}\\n'.format(train_labels))\n",
    "print('{}\\n'.format(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the train_test_split function randomly shuffles the dataset and corresponding labels prior to splitting. This is good practice to remove any systematic orderings in the dataset, which could potentially impact the model into training on the orderings rather than the actual data.\n",
    "\n",
    "The default size of the testing set is 25% of the original dataset. We can use the test_size keyword argument to manually specify the proportion of the original dataset that will go into the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[10.1 ,  0.4 ],\n",
      "       [ 8.3 ,  0.8 ],\n",
      "       [10.2 ,  0.5 ],\n",
      "       [ 9.3 ,  0.8 ],\n",
      "       [ 9.1 ,  0.68]])\n",
      "\n",
      "array([1.5, 1.2, 1.4, 1.6, 1.3])\n",
      "\n",
      "array([[7.7 , 0.9 ],\n",
      "       [8.7 , 0.9 ],\n",
      "       [9.5 , 0.77]])\n",
      "\n",
      "array([1.1, 1.2, 1.6])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = np.array([\n",
    "  [10.2 ,  0.5 ],\n",
    "  [ 8.7 ,  0.9 ],\n",
    "  [ 9.3 ,  0.8 ],\n",
    "  [10.1 ,  0.4 ],\n",
    "  [ 9.5 ,  0.77],\n",
    "  [ 9.1 ,  0.68],\n",
    "  [ 7.7 ,  0.9 ],\n",
    "  [ 8.3 ,  0.8 ]])\n",
    "labels = np.array(\n",
    "  [1.4, 1.2, 1.6, 1.5, 1.6, 1.3, 1.1, 1.2])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "split_dataset = train_test_split(data, labels,\n",
    "                                test_size=0.375)\n",
    "train_data = split_dataset[0]\n",
    "test_data = split_dataset[1]\n",
    "train_labels = split_dataset[2]\n",
    "test_labels = split_dataset[3]\n",
    "\n",
    "print('{}\\n'.format(repr(train_data)))\n",
    "print('{}\\n'.format(repr(train_labels)))\n",
    "print('{}\\n'.format(repr(test_data)))\n",
    "print('{}\\n'.format(repr(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In later chapters, we'll discuss how we use the testing set to evaluate a trained (fitted) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
